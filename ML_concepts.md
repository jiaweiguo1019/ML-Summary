# ML Concepts

## 绪论

### 基本术语

在计算机系统中，“经验”通常以“数据”形式存在，因此，机器学习所研究的主要内容，是关于在计算机上从数据中产生“模型”的算法，即学习算法（learning algorithm）。有了学习算法，我们把**经验**数据提供给`它`，它就能基于这些数据产生模型；在面对新的情况时，模型会给我们提供相应的判断。

数据集
示例、样本
属性、特征
属性值

属性张成的空间称为“属性空间”、“样本空间”或“输入空间”。
由于空间中的每个点对应一个坐标向量，因此我们也把**一个示例**称为一个“**特征向量**”。

从数据学得模型的过程称为“学习”或“训练”，这个过程是通过执行**某个**学习算法来完成。
学得模型对应了关于数据的某种潜在的规律，因此亦称“假设”；这种潜在规律自身，则称为“真相”或“真实”，学习过程就是为了找出或逼近真相。

标记
有了标记信息的示例，则称为“样例”
标记的集合：“样本空间”或“输出空间”

通常**假设**样本空间中全体样本服从一个未知的“分布” $\mathcal{D}$，我们获得的每个样本都是独立地从这个分布上采样获得的，即“独立同分布”

### 假设空间

归纳（induction）：特殊到一般
演绎（deduction）：一般到特殊

狭义的归纳学习：“概念学习”或“概念形成”

我们可以把学习过程看作一个在**所有**假设组成的空间中进行搜索的过程，搜索目标是找到与训练集“匹配”（fit）的假设。

现实问题中我们常面临很大的假设空间，但学习过程是基于有限样本训练集进行的，因为，可能有多个假设与训练集一致，即存在着一个与训练集一致的“假设集合”，我们称之为“**版本空间**”（version space）

### 归纳偏好

对于一个具体的学习算法，**必须**要产生一个模型。这时，学习算法本身的“偏好”就会起到关键的作用。

机器学习算法在学习过程中对某种类型假设的偏好，称为“归纳偏好”（inductive bias），或简称为“偏好”。
任何一个有效的机器学习算法必有其归纳偏好，否则它将被假设空间中看似在训练集上“等效”的假设所迷惑，而无法产生确定的学习结果。

奥卡姆剃刀本身存在不同的诠释，使用奥卡姆剃刀原则**并不平凡**。

NFL的前提：所有“问题”出现的机会相同、或所有问题同等重要。

## 模型评估与选择

### 评估方法

训练误差、经验误差：学习器在训练集上的误差
泛化误差：在新样本上的误差

评估方法：以测试集上的“测试误差”作为泛化误差的**近似**

#### 留出法

训练/测试集的划分要尽可能保持数据分布的一致性，避免因数据划分过程引入**额外的偏差**而对最终结果产生影响。
如果从采样的角度来看待数据集的划分过程，则**保留类别比例**的采样方式通常称为“分层采样”。
**单次**使用留出法得到的估计结果往往不够可靠，在使用留出法时，一般要采用**若干次**随机划分、重复进行实验评估后取平均值作为留出法的评估结果。

我们希望评估的是用 $D$ 训练出的模型的性能，但留出法需划分训练/测试集，这就会导致一个窘境：若令训练集 $S$ 包含绝大多数样本，则训练出得模型可能更接近于用 $D$ 训练出的模型，但由于 $T$ 比较小，评估结果可能不够稳定准确；若令测试集 $T$ 多包含一些样本，则训练集 $S$ 与 $D$ 差别更大了，被评估的模型与用 $D$ 训练出的模型相比可能有较大的差别，从而降低了评估结果的保真性（fidelity）。
> 可从“偏差-方差”的角度来理解：测试集小时，评估结果的方差较大；训练集小时，评估结果的偏差较大。

留一法：数据集比较大时，训练m个模型的计算开销可能是难以忍受的，而这还是在未考虑算法挑参的情况下。另外，留一法的估计结果也未必永远比其他评估方法准确；“没有免费的午餐”定理**对实验评估方法同样适用**。

#### 交叉验证

k-fold cross validation
交叉验证法评估结果的稳定性和保真性在很大程度上取决于k的取值
> “10次10折交叉验证法”与“100次留出法”都是进行了100次训练/测试。

#### 自助法

自助采样
给定包含 $m$ 个样本的数据集 $D$，我们对它进行采样产生数据集 $D^{'}$：每次随机从 $D$ 中挑选**一个**样本，将其**拷贝**放入 $D^{'}$，然后再将该样本**放回**初始数据集 $D$ 中，使得该样本在下次采样时仍有可能被采到；这个过程**重复执行** $m$ 次后，我们就得到了包含 $m$ 个样本的数据集 $D^{'}$。
显然，$D$ 中有一部分会在 $D^{'}$ 中多次出现，而另一部分样本不出现。
$\lim_{m \rarr \infin} (1 -  \frac{1}{m})^m = \frac{1}{e}$
即通过自助采样，初始数据集 $D$ 中约有 $36.8\%$ 的样本未出现在采样数据集 $D^{'}$ 中。 于是我们可将 $D^{'}$ 用做云莲集， $D \backslash D^{'}$ 用作测试集；这样，实际评估的模型与期望评估的模型**都使用m个训练样本**，而我们仍有数据总量约 $1/3$ 的、没在训练集中出现的样本用于测试。这样的测试结果，亦称“包外估计”（out-of-bag）。

自助法在**数据集小、难以有效划分训练/测试集**时很有用；此外，自助法**能从初始数据集中产生多个不同的训练集**，这对**集成学习**等方法有很大的好处。然而，自助法产生的数据集**改变了初始数据集的分布**，这会引入偏差。因此，在初始数据集足够时，留出法和交叉验证法更常用一些。

#### 调参与最终模型

给定包含 $m$ 个样本的数据集 $D$，在模型评估与选择过程中由于需要留出一部分数据进行评估测试，事实上我们只使用了一部分数据训练模型。因为，在模型选择完成后，学习算法和参数配置已选定，此时应该用数据集 $D$ **重新训练模型**。这个模型在训练过程中使用了**所有**的  $m$ 个样本，这才是我们最终提交给用户的模型。

### 性能度量

#### P-R曲线与F1

$F_{\beta} = \frac{(1 + \beta) \times P \times R}{(\beta^2 \times P) + R}$

其中 $\beta > 0$ 度量了查全率对查准率的相对重要性。$\beta = 1$ 时退化为标准的 $F1$；$\beta > 1$ 时查全率有更大影响；$\beta < 1$ 时查准率有更大影响。

在 $n$ 个二分类混淆矩阵上综合考察查准率和查全率。
一种直接的做法是先在各混淆矩阵上分别计算出查准率和查全率，再计算**平均值**，这样就得到了“宏查准率”（$macro-P$）、“宏查全率”（$macro-R$），以及相应的“宏$F1$”（$macro-F1$）:
$macro-P = \frac{1}{n} \sum_{i = 1}^{n} P_i$
$macro-R = \frac{1}{n} \sum_{i = 1}^{n} R_i$
$macro-F_1 = \frac{2 \times macro-P \times macro-R}{macro-P + macro-R}$

**还可**先将各混淆矩阵的**对应元素**进行平均，得到 $TP$、$FP$、$TN$、$FN$ 的平均值，分别记为 $\overline{TP}$、$\overline{FP}$、$\overline{TP}$、$\overline{FP}$ 的平均值，再基于这些平均值计算出“**微**查准率”（$micro-P$）、“微查全率”（$micro-R$），以及相应的“微$F1$”（$micro-F1$）:

$micro-P = \frac{\overline{TP}}{\overline{TP} + \overline{FP}}$
$micro-R = \frac{\overline{TP}}{\overline{TP} + \overline{FN}}$
$micro-F_1 = \frac{2 \times micro-P \times micro-R}{micro-P + micro-R}$

#### ROC 与 AUC

在不同的应用任务中，我们可根据任务需求来采用不同的截断点（cut point），例如若我们更重视“查准率”，则可选择排序中靠前的未知进行截断；若更重视“查全率”，则可选择靠后的位置进行截断。因此，排序本身的质量好坏，体现了综合考虑学习期在不同任务下的“期望泛化性能”的好坏，或者说，“一般情况下”泛化能力的好坏。ROC曲线则是从这个角度出发来研究学习器泛化性能的有力工具。

ROC（Receiver Operating Characteristic）：受试者工作特征

横轴“假正例率”（False Positive Rate，简称FPR），纵轴“真正例率”（True Positive Rate）

现实任务中通常是利用有限个测试样例来绘制 ROC 图
给定 $m^+$ 个正例和 $m^-$ 个反例，根据学习器预测结果对样例进行排序，然后把分类阈值设为最大，即把所有样例均预测为反例，此时真正例率和假正例率均为0，在坐标（0，0）处标记一个点。然后，将分类阈值依次设为每个样例的预测值，即**依次**将每个样例划分为*正例**。设前一个标记点坐标为（$x, y$），当前若为真正例，则对应标记点的坐标为（$x, y + \frac{1}{m^+}$）；当前若为假正例，则对应标记点的坐标为（$x + \frac{1}{m^-}, y$），然后用线段连接相邻点即得。

形式化地看，AUC考虑的是样本排序的排序质量，因此它与排序误差有紧密联系。给定 $m^+$ 个正例和 $m^-$ 个反例，令 $D^+$ 和 $D^-$ 分别表示正、反例集合，则排序“损失”（loss）定义为

$\ell_{rank} = \frac{1}{m^+ m^-} \displaystyle\sum_{\bm{x}^+ \in D^+}\displaystyle\sum_{\bm{x}^- \in D^-} \bigg( \mathbb{I} \big( f(\bm{x}^+) < f(\bm{x}^-) \big) + \frac{1}{2} \mathbb{I} \big( f(\bm{x}^+) = f(\bm{x}^-) \big) \bigg)$

即考虑**每一对**正、反例，若正例的预测值小雨反例，则记一个“发奋”，若相等，则记 $0.5$ 个罚分。容易看出，$\ell_{rank}$ 对应的是ROC曲线**之上**的面积：若一个正例在ROC曲线上对应标记点的坐标为（$x, y$），则 $x$ 恰是排序在其之前的反例所占的比例，即假正例率。因此有

$\text{AUC} = 1 - \ell_{rank}$

#### 代价曲线

为权衡不同类型错误所造成的不同损失，可为错误赋予“非均等代价”（unequal cost），则“代价敏感”（cost-sensitive）错误率为

$E(f; D; cost) = \frac{1}{m} \bigg( \displaystyle\sum_{\bm{x}_i \in D^+} \mathbb{I} \big(f(\bm{x}_i) \not = y_i \big) \times cost_{01} + \displaystyle\sum_{\bm{x}_i \in D^-} \mathbb{I} \big(f(\bm{x}_i) \not = y_i \big) \times cost_{10} \bigg)$

在非均等代价下，ROC曲线不能直接反应出学习器的期望总体代价，而“代价曲线”（cost curve）则可达到该目的。

### 比较检验

* 我们希望比较的是**泛化能力**，然而通过实验评估我们获得的是**测试集上的性能**，两者的对比结果可能未必相同
* 测试集上的性能与**测试集本身的选择**有很大关系，且不论使用不同大小的测试集会得到不同的结果，**即便**用相同大小的测试集，若包含的测试样例不同，测试结果也会有不同
* 很多机器学习算法本身包含一定的**随机性**，即便用相同的参数设置在同一个测试集上多次运行，其结果也会有不同。

统计**假设检验**（hypothesis test）为我们进行 学习器性能比较提供了重要依据。基于假设检验结果我们可推断出，**若**在测试集上观察到学习器A比B好，则A的泛化性能是否在**统计意义上**优于B，以及这个结论的把握有多大。

#### 偏差与方差

“偏差-方差分解”（bias-variance decomposition）试图对学习算法的**期望**泛化错误率进行拆解。我们知道，算法在不同训练集上学得的结果很可能不同，即便这些训练集是来自同一个分布。对测试样本 $\bm{x}$，令 $y_D$ 为 $\bm{x}$ 在数据集中的标记，$y$ 为  $\bm{x}$ 的真实标记，$f(\bm{x}; D)$ 为训练集$D$ 上学得模型 $f$ 在 $\bm{x}$ 上的预测输出



asdasffds
fdfgdf


gfgd
dfgd

